{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37d2a8f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chongxin/.local/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import dgl\n",
    "#import torchcde\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from pytorch_lightning import Trainer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import networkx as nx\n",
    "#import random\n",
    "import os\n",
    "import pandas as pd\n",
    "import torchcde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3230322d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://linqs-data.soe.ucsc.edu/public/lbc/cora.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "374e2c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_file = open('./cora/cora.cites', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed06c098",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_file.seek(0)\n",
    "cora_edgelist = []\n",
    "for line in graph_file.readlines():\n",
    "    i, j = line.split()\n",
    "    cora_edgelist.append((int(j),int(i)))  # Correct direction of links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c3e94ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "cora = nx.DiGraph(cora_edgelist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73990f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup = {}\n",
    "for new_ids, ids in enumerate(cora.nodes()):\n",
    "    lookup[ids] = new_ids\n",
    "# Create new graph with new node ids\n",
    "new_cora = nx.DiGraph()\n",
    "for i, j in cora.edges():\n",
    "    new_cora.add_edge(lookup[i], lookup[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24faa48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "content = './cora/cora.content'\n",
    "labels = {'Case_Based': 0, 'Genetic_Algorithms': 1, 'Neural_Networks': 2, \n",
    "          'Probabilistic_Methods': 3, 'Reinforcement_Learning':4, \n",
    "          'Rule_Learning': 5, 'Theory': 6}\n",
    "cora_labels = np.ndarray(shape=len(new_cora), dtype=int)\n",
    "cora_features = np.ndarray(shape=(len(new_cora), 1433), dtype=int)\n",
    "with open(content, 'r') as f:\n",
    "    for lines in f.readlines():\n",
    "        idx, *data, label = lines.strip().split()\n",
    "        idx = int(idx)\n",
    "        cora_labels[lookup[idx]] = labels[label]\n",
    "        for i, val in enumerate(map(int, data)):\n",
    "            cora_features[lookup[idx]][i] = val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6583cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "src = [i[0] for i in new_cora.edges()]\n",
    "dst = [i[1] for i in new_cora.edges()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7010e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dag = dgl.from_networkx(new_cora)\n",
    "cora_features = torch.tensor(cora_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6546e2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "g=dgl.add_reverse_edges(dag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347fcd8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "p=(g.in_degrees()/g.num_edges())**(3/4)\n",
    "p=p.numpy()/torch.sum(p).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8941ba22",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def compute_auc(pos_score, neg_score):\n",
    "    scores = torch.cat([pos_score, neg_score]).cpu().numpy()\n",
    "    labels = torch.cat(\n",
    "        [torch.ones(pos_score.shape[0]), torch.zeros(neg_score.shape[0])]).numpy()\n",
    "    return roc_auc_score(labels, scores)\n",
    "\n",
    "def seed(seed=43):\n",
    "    \"\"\"\n",
    "    Fix random process by a seed.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    #torch.cuda.manual_seed(seed)\n",
    "    #torch.cuda.manual_seed_all(seed)\n",
    "    #torch.backends.cudnn.deterministic = True\n",
    "    #torch.backends.cudnn.benchmark = False\n",
    "    dgl.random.seed(seed)\n",
    "\n",
    "#seed()\n",
    "print(torch.__version__)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be40898",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_edges=torch.stack(g.edges())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9544be34",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = np.arange(len(g_edges[0]))\n",
    "np.random.shuffle(ids)\n",
    "trainnum = int(len(ids)*0.8)\n",
    "train_id = ids[:trainnum]\n",
    "val_id = ids[trainnum:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c001427",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_size=128\n",
    "lr = 0.01\n",
    "def collate_fn(data):\n",
    "    bs = len(data)\n",
    "    edges=torch.cat([g_edges[:,data],g_edges[[1,0],:][:,data]],dim=1)\n",
    "    src = edges[0,:].repeat(15)\n",
    "    edges=set([i for i in edges.permute((1,0))])\n",
    "    \n",
    "    dst = torch.tensor(np.random.choice(np.arange(g.num_nodes()),p=p,size=30*bs))\n",
    "    \n",
    "    neg_data = torch.stack([src,dst])\n",
    "    neg_data = torch.stack([i for i in neg_data.permute((1,0)) if i not in edges])\n",
    "    \n",
    "    data = torch.stack(list(edges))\n",
    "    \n",
    "    return data, neg_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc491c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class skipgram(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, dim_emb=64, word_count=0, batch_size=seed_size):\n",
    "        super(skipgram, self).__init__()\n",
    "        self.v = nn.Embedding(word_count, dim_emb)\n",
    "        self.u = nn.Embedding(word_count, dim_emb)\n",
    "        self.init_weight\n",
    "        self.batch_size = batch_size\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def init_weight(self):\n",
    "        gain = 1 #0.0036\n",
    "        nn.init.xavier_uniform_(self.v.weight, gain=gain)\n",
    "        nn.init.xavier_uniform_(self.u.weight, gain=gain)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.u(x), self.v(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "\n",
    "        data, neg_data = batch[0], batch[1]\n",
    "        \n",
    "        u_emb = self.u(data[:,0]).unsqueeze(1)\n",
    "        v_emb = self.v(data[:,1]).unsqueeze(-1)\n",
    "        neg_u_emb = self.u(neg_data[:,0]).unsqueeze(1)\n",
    "        neg_v_emb = self.v(neg_data[:,1]).unsqueeze(-1)\n",
    "        \n",
    "        score = torch.bmm(u_emb, v_emb).squeeze(1)\n",
    "        neg_score = torch.bmm(neg_u_emb, neg_v_emb).squeeze(1)\n",
    "\n",
    "        score = torch.cat((F.logsigmoid(score),F.logsigmoid(-neg_score)))\n",
    "        loss = -torch.mean(score)\n",
    "\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "\n",
    "        data, neg_data = batch[0], batch[1]\n",
    "        \n",
    "        u_emb = self.u(data[:,0]).unsqueeze(1)\n",
    "        v_emb = self.v(data[:,1]).unsqueeze(-1)\n",
    "        neg_u_emb = self.u(neg_data[:,0]).unsqueeze(1)\n",
    "        neg_v_emb = self.v(neg_data[:,1]).unsqueeze(-1)\n",
    "        \n",
    "        score = torch.bmm(u_emb, v_emb).squeeze(1)\n",
    "        neg_score = torch.bmm(neg_u_emb, neg_v_emb).squeeze(1)\n",
    "\n",
    "        score = torch.cat((F.logsigmoid(score),F.logsigmoid(-neg_score)))\n",
    "        loss = -torch.mean(score)\n",
    "        auc=compute_auc(score,neg_score)\n",
    "        \n",
    "        print('epoch: ', self.current_epoch, 'val_loss: ', loss.detach().numpy(),'auc:',auc)\n",
    "        self.log('val_loss', loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "\n",
    "        data, neg_data = batch[0], batch[1]\n",
    "        \n",
    "        u_emb = self.u(data[:,0]).unsqueeze(1)\n",
    "        v_emb = self.v(data[:,1]).unsqueeze(-1)\n",
    "        neg_u_emb = self.u(neg_data[:,0]).unsqueeze(1)\n",
    "        neg_v_emb = self.v(neg_data[:,1]).unsqueeze(-1)\n",
    "        \n",
    "        score = torch.bmm(u_emb, v_emb).squeeze(1)\n",
    "        neg_score = torch.bmm(neg_u_emb, neg_v_emb).squeeze(1)\n",
    "\n",
    "        score = torch.cat((F.logsigmoid(score),F.logsigmoid(-neg_score)))\n",
    "        loss = -torch.mean(score)\n",
    "\n",
    "        return test_loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=lr, weight_decay=1e-6)\n",
    "        return optimizer\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(ids, batch_size=self.batch_size, shuffle=True, collate_fn=collate_fn, num_workers=0, drop_last=True, pin_memory=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(val_id, batch_size=len(val_id), shuffle=False, collate_fn=collate_fn, num_workers=0, drop_last=True, pin_memory=True)\n",
    "\n",
    "    def predict_dataloader(self):\n",
    "        return DataLoader(ids, batch_size=g.num_nodes(), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf22d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = skipgram(dim_emb=16, word_count=g.num_nodes())\n",
    "trainer = Trainer(min_epochs=0,\n",
    "                  max_epochs=60,\n",
    "                  check_val_every_n_epoch=10,\n",
    "                  progress_bar_refresh_rate=1,\n",
    "                  gpus=0,\n",
    "                  reload_dataloaders_every_n_epochs=5,\n",
    "                  profiler='simple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f130320d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f4141c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#output = model.v(torch.arange(g.num_nodes())).detach()\n",
    "#np.save('cora_embedding.npy', output)\n",
    "output=torch.tensor(np.load('cora_embedding.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e9d5cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "_,_,V = torch.pca_lowrank(cora_features.float(),q=15,center=True, niter=100)\n",
    "cora_features = torch.matmul(cora_features.float(), V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "13f64349",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn(data):\n",
    "    dst = data\n",
    "    trials=20\n",
    "    length=7\n",
    "    paths=[]\n",
    "    ids=data.repeat(trials)\n",
    "    rw = dgl.sampling.random_walk(dag, ids, length=length)[0]\n",
    "    visited=set()\n",
    "    for i in range(length+1):\n",
    "        for j in range(trials):\n",
    "            if rw[j,i].item()>=0:\n",
    "                if rw[j,i].item() in visited:\n",
    "                    rw[j,i:]=-1\n",
    "                    break\n",
    "        for j in range(trials):\n",
    "            if rw[j,i].item()>=0:\n",
    "                visited.add(rw[j,i].item())\n",
    "    for i in range(trials):\n",
    "        for j in range(length+1):\n",
    "            if rw[i,j]==-1 and j>1:\n",
    "                if list(rw[i,:j].numpy()) not in paths:\n",
    "                    paths.append(list(rw[i,:j].numpy()))\n",
    "                break\n",
    "    length = max([len(p) for p in paths])\n",
    "    bs = len(paths)\n",
    "    \n",
    "    for i in range(len(paths)):\n",
    "        paths[i] = [dst.item()]*(length-len(paths[i]))+paths[i]\n",
    "        \n",
    "    paths = torch.tensor(paths).long()\n",
    "    cora_feat = cora_features[torch.flip(paths,dims=(-1,)).long()]\n",
    "    paths     =        output[torch.flip(paths,dims=(-1,)).long()]\n",
    "    \n",
    "    time=torch.arange(paths.shape[1]).repeat(paths.shape[0],1).unsqueeze(-1)/8\n",
    "    X = torch.cat([time, paths,cora_feat], dim=-1)\n",
    "    train_coeffs = torchcde.hermite_cubic_coefficients_with_backward_differences(X)\n",
    "    return cora_labels[dst], train_coeffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e03932d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(data):\n",
    "    def fn(data):\n",
    "        dst = data\n",
    "        trials=20\n",
    "        length=7\n",
    "        paths=[]\n",
    "        ids=data.repeat(trials)\n",
    "        rw = dgl.sampling.random_walk(dag, ids, length=length)[0]\n",
    "        visited=set()\n",
    "        for i in range(length+1):\n",
    "            for j in range(trials):\n",
    "                if rw[j,i].item()>=0:\n",
    "                    if rw[j,i].item() in visited:\n",
    "                        rw[j,i:]=-1\n",
    "                        break\n",
    "            for j in range(trials):\n",
    "                if rw[j,i].item()>=0:\n",
    "                    visited.add(rw[j,i].item())\n",
    "        for i in range(trials):\n",
    "            for j in range(length+1):\n",
    "                if rw[i,j]==-1 and j>1:\n",
    "                    if list(rw[i,:j].numpy()) not in paths:\n",
    "                        paths.append(list(rw[i,:j].numpy()))\n",
    "                    break\n",
    "        length = max([len(p) for p in paths])\n",
    "        bs = len(paths)\n",
    "    \n",
    "        for i in range(len(paths)):\n",
    "            paths[i] = [dst.item()]*(length-len(paths[i]))+paths[i]\n",
    "        \n",
    "        paths = torch.tensor(paths).long()\n",
    "        cora_feat = cora_features[torch.flip(paths,dims=(-1,)).long()]\n",
    "        paths     =        output[torch.flip(paths,dims=(-1,)).long()]\n",
    "    \n",
    "        time=torch.arange(paths.shape[1]).repeat(paths.shape[0],1).unsqueeze(-1)/8\n",
    "        X = torch.cat([time, paths,cora_feat], dim=-1)\n",
    "        train_coeffs = torchcde.hermite_cubic_coefficients_with_backward_differences(X)\n",
    "        return cora_labels[dst], train_coeffs\n",
    "    batch = {i:fn(i) for i in data}\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c14ce94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = np.arange(dag.num_nodes())\n",
    "np.random.shuffle(ids)\n",
    "trainnum = int(len(ids)*0.8)\n",
    "train_id = ids[:trainnum]\n",
    "val_id = ids[trainnum:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "45f76426",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CDEFunc(torch.nn.Module):\n",
    "    def __init__(self, input_channels=32, hidden_channels=32):\n",
    "        ######################\n",
    "        # input_channels is the number of input channels in the data X. (Determined by the data.)\n",
    "        # hidden_channels is the number of channels for z_t. (Determined by you!)\n",
    "        ######################\n",
    "        super(CDEFunc, self).__init__()\n",
    "        self.input_channels = input_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "\n",
    "        self.linear1 = torch.nn.Linear(hidden_channels, 64)\n",
    "        self.linear2 = torch.nn.Linear(64, input_channels * hidden_channels)\n",
    "        \n",
    "        gain = 1 #0.0036\n",
    "        nn.init.xavier_uniform_(self.linear1.weight, gain=gain)\n",
    "        nn.init.xavier_uniform_(self.linear2.weight, gain=gain)\n",
    "\n",
    "    ######################\n",
    "    # For most purposes the t argument can probably be ignored; unless you want your CDE to behave differently at\n",
    "    # different times, which would be unusual. But it's there if you need it!\n",
    "    ######################\n",
    "    def forward(self, t, z):\n",
    "        # z has shape (batch, hidden_channels)\n",
    "        #print(z.shape)\n",
    "        z = self.linear1(z)\n",
    "        z = z.relu()\n",
    "        z = self.linear2(z)\n",
    "        ######################\n",
    "        # Easy-to-forget gotcha: Best results tend to be obtained by adding a final tanh nonlinearity.\n",
    "        ######################\n",
    "        z = z.tanh()\n",
    "        ######################\n",
    "        # Ignoring the batch dimension, the shape of the output tensor must be a matrix,\n",
    "        # because we need it to represent a linear map from R^input_channels to R^hidden_channels.\n",
    "        ######################\n",
    "        z = z.view(z.size(0), self.hidden_channels, self.input_channels)\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "51aaaf61",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "crit = nn.CrossEntropyLoss()\n",
    "class CDE(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, input_dim=32, h_dim=32, output_dim=8, cora_features=cora_features, output=output):\n",
    "        super(CDE, self).__init__()\n",
    "        self.cdef=CDEFunc()\n",
    "        self.initial = torch.nn.Linear(input_dim, h_dim)\n",
    "        self.transform = torch.nn.Linear(input_dim, h_dim)\n",
    "        self.k = torch.nn.Linear(h_dim, h_dim)\n",
    "        self.q = torch.nn.Linear(h_dim, h_dim)\n",
    "        self.readout = torch.nn.Linear(h_dim, output_dim)\n",
    "        self.init_weight\n",
    "        self.prelu = nn.PReLU()\n",
    "        self.cora_features, self.output=cora_features, output\n",
    "        self.sm = nn.Softmax(dim=0)\n",
    "        #self.eps = nn.Parameter(torch.FloatTensor([1.0]))\n",
    "\n",
    "    def init_weight(self):\n",
    "        gain = 1 #0.0036\n",
    "        nn.init.xavier_uniform_(self.initial.weight, gain=gain)\n",
    "        nn.init.xavier_uniform_(self.readout.weight, gain=gain)\n",
    "        nn.init.xavier_uniform_(self.k.weight, gain=gain)\n",
    "        nn.init.xavier_uniform_(self.q.weight, gain=gain)\n",
    "        nn.init.xavier_uniform_(self.transform.weight, gain=gain)\n",
    "        \n",
    "\n",
    "    def forward(self, data, batch_idx):\n",
    "        cora_features, output=self.cora_features, self.output\n",
    "        pred = []\n",
    "        Y = []\n",
    "        for i in data:\n",
    "            y, X = data[i]\n",
    "            Y.append(y)\n",
    "            #print(X.shape)\n",
    "            X = torchcde.CubicSpline(X)\n",
    "            X0 = X.evaluate(X.interval[0])\n",
    "            #print('X0:',X0.shape)\n",
    "            z0 = self.initial(X0)\n",
    "            z_T = torchcde.cdeint(X=X,\n",
    "                              z0=z0,\n",
    "                              func=self.cdef,\n",
    "                              t=X.interval)\n",
    "            z_T = z_T[:, 1]\n",
    "            #print(z_T.shape)\n",
    "            dst_feat = self.transform(torch.cat([z_T[0,0].unsqueeze(0),output[i],cora_features[i]]))\n",
    "            #print(dst_feat.shape)\n",
    "            z_T = torch.cat([dst_feat.unsqueeze(0),z_T])\n",
    "            #print(z_T.shape)\n",
    "            q = self.q(dst_feat)\n",
    "            k = self.k(z_T)\n",
    "            h = torch.sum(z_T* self.sm(torch.matmul(q, k.t())).unsqueeze(-1),dim=0)\n",
    "            pred.append(self.readout(h))\n",
    "        return data, torch.stack(pred), torch.tensor(Y)\n",
    "    \n",
    "    def training_step(self, data, batch_idx):\n",
    "        cora_features, output=self.cora_features, self.output\n",
    "        pred = []\n",
    "        Y = []\n",
    "        for i in data:\n",
    "            y, X = data[i]\n",
    "            Y.append(y)\n",
    "            X = torchcde.CubicSpline(X)\n",
    "            X0 = X.evaluate(X.interval[0])\n",
    "            z0 = self.initial(X0)\n",
    "            z_T = torchcde.cdeint(X=X,\n",
    "                              z0=z0,\n",
    "                              func=self.cdef,\n",
    "                              t=X.interval)\n",
    "            z_T = z_T[:, 1]\n",
    "            #print(z_T.shape)\n",
    "            dst_feat = self.transform(torch.cat([z_T[0,0].unsqueeze(0),output[i],cora_features[i]]))\n",
    "            #print(dst_feat.shape)\n",
    "            z_T = torch.cat([dst_feat.unsqueeze(0),z_T])\n",
    "            #print(z_T.shape)\n",
    "            q = self.q(dst_feat)\n",
    "            k = self.k(z_T)\n",
    "            h = torch.sum(z_T* self.sm(torch.matmul(q, k.t())).unsqueeze(-1),dim=0)\n",
    "            pred.append(self.readout(h))\n",
    "        loss = crit(torch.stack(pred), torch.tensor(Y))\n",
    "        pred = torch.argmax(torch.stack(pred),dim=-1)\n",
    "        acc = torch.mean((pred==torch.tensor(Y)).float()) \n",
    "        return loss\n",
    "    def validation_step(self, data, batch_idx):\n",
    "        cora_features, output=self.cora_features, self.output\n",
    "        pred = []\n",
    "        Y = []\n",
    "        for i in data:\n",
    "            y, X = data[i]\n",
    "            Y.append(y)\n",
    "            X = torchcde.CubicSpline(X)\n",
    "            X0 = X.evaluate(X.interval[0])\n",
    "            z0 = self.initial(X0)\n",
    "            z_T = torchcde.cdeint(X=X,\n",
    "                              z0=z0,\n",
    "                              func=self.cdef,\n",
    "                              t=X.interval)\n",
    "            z_T = z_T[:, 1]\n",
    "            dst_feat = self.transform(torch.cat([z_T[0,0].unsqueeze(0),output[i],cora_features[i]]))\n",
    "            #print(dst_feat.shape)\n",
    "            z_T = torch.cat([dst_feat.unsqueeze(0),z_T])\n",
    "            #print(z_T.shape)\n",
    "            q = self.q(dst_feat)\n",
    "            k = self.k(z_T)\n",
    "            h = torch.sum(z_T* self.sm(torch.matmul(q, k.t())).unsqueeze(-1),dim=0)\n",
    "            pred.append(self.readout(h))\n",
    "        loss = crit(torch.stack(pred), torch.tensor(Y))\n",
    "        pred = torch.argmax(torch.stack(pred),dim=-1)\n",
    "        acc = torch.mean((pred==torch.tensor(Y)).float())\n",
    "        \n",
    "        print('epoch: ', self.current_epoch, 'val_loss: ', loss.detach().numpy(),'acc:', acc.item())\n",
    "        return loss\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=lr, weight_decay=1e-6)\n",
    "        return optimizer\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(train_id, batch_size=16, shuffle=True, collate_fn=collate_fn, num_workers=0, pin_memory=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(val_id, batch_size=64, shuffle=False, collate_fn=collate_fn, num_workers=0, pin_memory=True)\n",
    "\n",
    "    def predict_dataloader(self):\n",
    "        return DataLoader(ids, batch_size=g.num_nodes(), shuffle=False)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b2c57963",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "model=CDE()\n",
    "trainer = Trainer(min_epochs=0,\n",
    "                  max_epochs=80,\n",
    "                  check_val_every_n_epoch=1,\n",
    "                  progress_bar_refresh_rate=0,\n",
    "                  gpus=0,\n",
    "                  reload_dataloaders_every_n_epochs=5,\n",
    "                  profiler='simple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25aa321a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name      | Type    | Params\n",
      "--------------------------------------\n",
      "0 | cdef      | CDEFunc | 68.7 K\n",
      "1 | initial   | Linear  | 1.1 K \n",
      "2 | transform | Linear  | 1.1 K \n",
      "3 | k         | Linear  | 1.1 K \n",
      "4 | q         | Linear  | 1.1 K \n",
      "5 | readout   | Linear  | 264   \n",
      "6 | prelu     | PReLU   | 1     \n",
      "7 | sm        | Softmax | 0     \n",
      "--------------------------------------\n",
      "73.2 K    Trainable params\n",
      "0         Non-trainable params\n",
      "73.2 K    Total params\n",
      "0.293     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0 val_loss:  2.2166483 acc: 0.0625\n",
      "epoch:  0 val_loss:  2.2360587 acc: 0.125\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c646845",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader=DataLoader(train_id, batch_size=16, shuffle=True, collate_fn=collate_fn, num_workers=0, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd2853d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a25fdde",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
